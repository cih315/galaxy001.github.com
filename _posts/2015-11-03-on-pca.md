---
layout: post
date: 'Tue 2015-11-03 19:05:36 +0800'
slug: "on-pca"
title: "On PCA"
description: ""
category: Bioinformatics
tags: Biology, Galaxy_Original
---
{% include JB/setup %}

# Introduction to PCA

{:toc}

## Types of PCA in Biology

1. For `Quantitative traits`
2. In `Population`/`Micro Evolution` Study
3. In `Macro Evolution` Study

### *Population* Case

PCA in `smartpca` from [EIGENSOFT](http://genetics.med.harvard.edu/reich/Reich_Lab/Software.html):

> ![The EIGENSTRAT algorithm](http://www.nature.com/ng/journal/v38/n8/images/ng1847-F1.jpg)

> For individuals from population \\(l\\) with population allele frequency \\(p_l\\), control individuals were assigned genotype 0, 1 or 2 with probabilities \\((1 - p_l)^2, 2p_l(1 - p_l)\\), or \\(p_l^2\\), respectively.
> 
> *Price, A. L., Patterson, N. J., Plenge, R. M., Weinblatt, M. E., Shadick, N. A., & Reich, D. (2006). Principal components analysis corrects for stratification in genome-wide association studies. Nature Genetics, 38(8), 904–9. doi:10.1038/ng1847*

EIGENSTRAT use `0`,`2` for two homozygous alleles and `1` for heterozygous sites.

> The EIGENSOFT package combines functionality from our population genetics methods ([Patterson et al. 2006](http://www.plosgenetics.org/article/info%3Adoi%2F10.1371%2Fjournal.pgen.0020190)) and our EIGENSTRAT stratification correction method ([Price et al. 2006](http://www.nature.com/ng/journal/v38/n8/abs/ng1847.html)). The EIGENSTRAT method uses principal components analysis to explicitly model ancestry differences between cases and controls along continuous axes of variation; the resulting correction is specific to a candidate marker’s variation in frequency across ancestral populations, minimizing spurious associations while maximizing power to detect true associations. The EIGENSOFT package has a built-in plotting script and supports multiple file formats and quantitative phenotypes. 

### *Macro Evolution* Case

PCA in [jalview](http://www.jalview.org/help/html/calculations/pca.html) states:

> *Calculating PCAs for aligned sequences*  
> Jalview can perform PCA analysis on both proteins and nucleotide sequence alignments. In both cases, components are generated by an eigenvector decomposition of the matrix formed from the sum of substitution matrix scores at each aligned position between each pair of sequences - computed with one of the available score matrices, such as [BLOSUM62](http://www.jalview.org/help/html/calculations/scorematrices.html#blosum62), [PAM250](http://www.jalview.org/help/html/calculations/scorematrices.html#pam250), or the [simple single nucleotide substitution matrix](http://www.jalview.org/help/html/calculations/scorematrices.html#simplenucleotide).
> 
> ![Principal Component Analysis, Menu](http://www.jalview.org/help/html/calculations/pcaviewer.gif)

Codes at [[jalview.git]](http://source.jalview.org/gitweb/?p=jalview.git) / [src / jalview / analysis / PCA.java](http://source.jalview.org/gitweb/?p=jalview.git;a=blob;f=src/jalview/analysis/PCA.java)

## Principal component Algorithms

### EIG 特征值分解

Eigenvalue decomposition (EIG) of the covariance matrix.

`?princomp`  
The calculation is done using ‘eigen’ on the correlation or covariance matrix, as determined by ‘cor’.  This is done for compatibility with the S-PLUS result.  A preferred method of calculation is to use ‘svd’ on ‘x’, as is done in ‘prcomp’.

### SVD 奇异值分解

Singular value decomposition (SVD) of X.

`?prcomp`  
The calculation is done by a singular value decomposition of the (centered and possibly scaled) data matrix, not by using ‘eigen’ on the covariance matrix.  This is generally the preferred method for numerical accuracy.

### EIG vs SVD ? SVD !

> The EIG algorithm is faster than SVD when the number of observations, n, exceeds the number of variables, p, but is less accurate because the condition number of the covariance is the square of the condition number of X.  
> --<http://www.mathworks.com/help/stats/pca.html#namevaluepairs>

------

> [Why PCA of data by means of SVD of the data?](http://stats.stackexchange.com/questions/79043) -- a discussion of what are the benefits of performing PCA via SVD [short answer: numerical stability].  
> --<http://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca>

## etc

* [LDA和PCA](http://www.ramlinbird.com/lda%E5%92%8Cpca/)
* https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/
* https://github.com/Ramlinbird/prml
* porly的博文——[LDA线性判别分析](http://blog.csdn.net/porly/article/details/8020696)
* LeftNotEasy的博文——[机器学习中的数学(4)：线性判别分析、主成分分析](http://blog.jobbole.com/88195/)
* 攻城狮凌风——[线性判别分析LDA详解](http://blog.csdn.net/qianhen123/article/details/39832951)
* 小林子——[四大机器学习降维算法：PCA、LDA、LLE、Laplacian Eigenmaps](http://dataunion.org/13451.html)

关于LDA，最近学习scikit_learn发现，原来存在两种推导方式——the Bayesian approach and the Fisher’s approach。两者推导过程非常不同，而且结果也存在一些差异。详情可查阅以下链接：  
http://stats.stackexchange.com/questions/87975/bayesian-and-fishers-approaches-to-linear-discriminant-analysis

http://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html

http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html

<https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/>

<http://www.isogg.org/wiki/Admixture_analyses>

[Singular value decomposition and principal component analysis (PDF)](http://arxiv.org/pdf/physics/0208101.pdf)

<http://stats.stackexchange.com/questions/35561/imputation-of-missing-values-for-pca>

<http://menugget.blogspot.com/2014/09/pca-eof-for-data-with-missing-values.html>
